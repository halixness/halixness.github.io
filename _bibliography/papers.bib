---
---

@string{aps = {American Physical Society,}}

/* ---------------
    Selected
   ---------------
*/
@article{CalanzoneVergari2024-2,
  abbr={Reasoning},
  title={Logically Consistent Language Models via Neuro-Symbolic Integration},
  journal={NeurIPS 2024 Workshop on System 2 Reasoning at Scale},
  author={Calanzone, Diego and Teso, Stefano and Vergari, Antonio},
  abstract={Large language models (LLMs) are a promising venue for natural language understanding and generation. However, current LLMs are far from reliable: they are prone to generating non-factual information and, more crucially, to contradicting themselves when prompted to reason about relations between entities of the world. These problems are currently addressed with large scale fine-tuning or by delegating reasoning to external tools. In this work, we strive for a middle ground and introduce a loss based on neuro-symbolic reasoning that teaches an LLM to be logically consistent with an external set of facts and rules and improves self-consistency even when the LLM is fine-tuned on a limited set of facts. Our approach also allows to easily combine multiple logical constraints at once in a principled way, delivering LLMs that are more consistent w.r.t. all constraints and improve over several baselines w.r.t. a given constraint. Moreover, our method allows LLMs to extrapolate to unseen but semantically similar factual knowledge, represented in unseen datasets, more systematically.},
  doi={10.48550/arXiv.2409.13724},
  url={https://arxiv.org/abs/2409.13724},
  selected={true}
}

@article{CalanzoneVergari2024,
  abbr={Reasoning},
  title={Towards Logically Consistent Language Models via Probabilistic Reasoning},
  journal={ICLR 2024 Workshop on Reliable and Responsible Foundation Models},
  author={Calanzone, Diego and Teso, Stefano and Vergari, Antonio},
  abstract={Large language models (LLMs) are a promising venue for natural language understanding and generation tasks. However, current LLMs are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about beliefs of the world. These problems are currently addressed with large scale fine-tuning or by delegating consistent reasoning to external tools. In this work, we strive for a middle ground and introduce a training objective based on principled probabilistic reasoning that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules. Fine-tuning with our loss on a limited set of facts enables our LLMs to be more logically consistent than previous baselines and allows them to extrapolate to unseen but semantically similar factual knowledge more systematically.},
  doi={10.48550/arXiv.2404.12843},
  url={https://arxiv.org/abs/2404.12843},
  selected={true}
}

@article{CalanzoneCasonato2024,
  abbr={AI Policy},
  title={An open source perspective on AI and alignment with the EU AI Act},
  journal={AISafety/SafeRL@ IJCAI 2023},
  author={Calanzone, Diego and Coppari, Andrea and Tedoldi, Riccardo and Olivato, Giulia and Casonato, Carlo},
  abstract={Artificial intelligence systems based on deep learning have increasingly received interest due to their success in complex human tasks. A current trend in deep learning is to study how algorithms learn multiple new abilities as their size and training data increase." General purpose AI"(GPAI), that is systems that can transfer the acquired knowledge to solve multiple tasks, are candidate to constitute the backbone of many AI algorithms applied in specific fields on industry, eg healthcare, customer support, administration. While various research laboratories express safety concerns on GPAI and do not openly share access to their algorithms, others advocate for their" democratization" and an increasing amount of open-source versions is available online. In this study we analyze this phenomenon from two perspectives and try to reconcile them. From one side, research communities support open collaborations, free access to knowledge and resources; on the other, political institutions, involved in the orchestration between the support for innovation and the control of societal impact, aim at preventing violations of fundamental human rights. We particularly focus on the European approach for risk assessment of AI systems. In our opinion, it greatly overlaps with work in ethics and law conducted by AI researchers (eg the Stanford Centre for Research on Foundation Models). Specifically we identify some necessary modifications to improve coordination between the two sides, while also discussing viable implementations in the technical field.},
  doi={},
  url={https://ceur-ws.org/Vol-3505/paper_10.pdf},
  selected={true}
}

https://xcorr.net/2022/05/18/whats-the-endgame-of-neuroai/

/* ---------------
    Publications
   ---------------
*/


@article{InteractiveAgents2022,
  abbr={Multi-modal},
  title={Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback},
  journal={Abramson et al.},
  author={Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, },
  abstract={Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, George Powell, Adam Santoro, Guy Scully, Sanjana Srivastava, Tamara von Glehn, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu},
  year={2022},
  doi={10.48550/arXiv.2211.11602},
  url={https://arxiv.org/abs/2211.11602}
}
@article{ColdDiffusionGoldStein2022,
  abbr={Generative DL},
  title={Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  journal={Bansal, Goldstein et al.},
  author={Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein},
  abstract={Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes.},
  year={2022},
  doi={10.48550/arXiv.2208.09392},
  url={https://arxiv.org/abs/2208.09392}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
