---
---

@string{aps = {American Physical Society,}}

/* ---------------
    Selected
   ---------------
*/
@article{CalanzoneVergari2024-2,
  abbr={Reasoning},
  title={Logically Consistent Language Models via Neuro-Symbolic Integration},
  journal={NeurIPS 2024 Workshop on System 2 Reasoning at Scale},
  author={Calanzone, Diego and Teso, Stefano and Vergari, Antonio},
  abstract={Large language models (LLMs) are a promising venue for natural language understanding and generation. However, current LLMs are far from reliable: they are prone to generating non-factual information and, more crucially, to contradicting themselves when prompted to reason about relations between entities of the world. These problems are currently addressed with large scale fine-tuning or by delegating reasoning to external tools. In this work, we strive for a middle ground and introduce a loss based on neuro-symbolic reasoning that teaches an LLM to be logically consistent with an external set of facts and rules and improves self-consistency even when the LLM is fine-tuned on a limited set of facts. Our approach also allows to easily combine multiple logical constraints at once in a principled way, delivering LLMs that are more consistent w.r.t. all constraints and improve over several baselines w.r.t. a given constraint. Moreover, our method allows LLMs to extrapolate to unseen but semantically similar factual knowledge, represented in unseen datasets, more systematically.},
  doi={10.48550/arXiv.2409.13724},
  url={https://arxiv.org/abs/2409.13724},
  selected={true}
}

@article{GoldszalCalanzoneTabogaBacon2025,
  abbr={AI4Science},
  title={Discovery of Sustainable Refrigerants through Physics-Informed RL Fine-Tuning of Sequence Models},
  journal={arXiv preprint arXiv:2509.19588},
  author={Goldszal, Adrien and Calanzone, Diego and Taboga, Vincent and Bacon, Pierre-Luc},
  abstract={We propose a method to discover environmentally sustainable refrigerant molecules by combining physics-informed predictors and reinforcement learning fine-tuning of sequence models. Our approach constrains the search via thermodynamic, safety, and environmental criteria, filtering candidate molecules to only those satisfying multiple rigorous properties. This enables identification of promising refrigerants despite scarce ground-truth data, and supports generalization beyond the training set.},
  doi={10.48550/arXiv.2509.19588},
  url={https://arxiv.org/abs/2509.19588},
  selected={true}
}

@article{CalanzoneDOroBacon2025,
  abbr={AI4Science},
  title={Mol-MoE: Training Preference-Guided Routers for Molecule Generation},
  journal={arXiv preprint arXiv:2502.05633},
  author={Calanzone, Diego and D’Oro, Pierluca and Bacon, Pierre-Luc},
  abstract={Recent advances in language models have enabled framing molecule generation as sequence modeling. However, existing approaches often rely on single-objective reinforcement learning, limiting their applicability to real-world drug design, where multiple competing properties must be optimized. Traditional multi-objective reinforcement learning (MORL) methods require costly retraining for each new objective combination, making rapid exploration of trade-offs impractical. To overcome these limitations, we introduce Mol-MoE, a mixture-of-experts (MoE) architecture that enables efficient test-time steering of molecule generation without retraining. Central to our approach is a preference-based router training objective that incentivizes the router to combine experts in a way that aligns with user-specified trade-offs. This provides improved flexibility in exploring the chemical property space at test time, facilitating rapid trade-off exploration. Benchmarking against state-of-the-art methods, we show that Mol-MoE achieves superior sample quality and steerability.},
  doi={10.48550/arXiv.2502.05633},
  url={https://arxiv.org/abs/2502.05633},
  selected={true}
}




https://xcorr.net/2022/05/18/whats-the-endgame-of-neuroai/

/* ---------------
    Publications
   ---------------
*/


@article{InteractiveAgents2022,
  abbr={Multi-modal},
  title={Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback},
  journal={Abramson et al.},
  author={Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, },
  abstract={Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, George Powell, Adam Santoro, Guy Scully, Sanjana Srivastava, Tamara von Glehn, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu},
  year={2022},
  doi={10.48550/arXiv.2211.11602},
  url={https://arxiv.org/abs/2211.11602}
}
@article{ColdDiffusionGoldStein2022,
  abbr={Generative DL},
  title={Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  journal={Bansal, Goldstein et al.},
  author={Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein},
  abstract={Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes.},
  year={2022},
  doi={10.48550/arXiv.2208.09392},
  url={https://arxiv.org/abs/2208.09392}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
