@ARTICLE{Lamport1986,
   author = {L[eslie] A. Lamport},
   title = {The Gnats and Gnus Document Preparation System},
   journal = {G-Animal's Journal},
   year = 1986,
   volume = 41,
   number = 7,
   pages = "73+",
   month = jul,
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2022emergent,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Stephen2010AnIT,
  title={An Introduction to Textual Econometrics},
  author={Fagan Stephen and Gençay Ramazan},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:168495426},
    pages={139}
}

@ARTICLE{1056813,
  author={Chomsky, N.},
  journal={IRE Transactions on Information Theory}, 
  title={Three models for the description of language}, 
  year={1956},
  volume={2},
  number={3},
  pages={113-124},
  doi={10.1109/TIT.1956.1056813}}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{BINDER2011527,
title = {The neurobiology of semantic memory},
journal = {Trends in Cognitive Sciences},
volume = {15},
number = {11},
pages = {527-536},
year = {2011},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2011.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661311002142},
author = {Jeffrey R. Binder and Rutvik H. Desai},
abstract = {Semantic memory includes all acquired knowledge about the world and is the basis for nearly all human activity, yet its neurobiological foundation is only now becoming clear. Recent neuroimaging studies demonstrate two striking results: the participation of modality-specific sensory, motor, and emotion systems in language comprehension, and the existence of large brain regions that participate in comprehension tasks but are not modality-specific. These latter regions, which include the inferior parietal lobe and much of the temporal lobe, lie at convergences of multiple perceptual processing streams. These convergences enable increasingly abstract, supramodal representations of perceptual experience that support a variety of conceptual functions including object recognition, social cognition, language, and the remarkable human capacity to remember the past and imagine the future.}
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@Article{1979-13045-001,
    author={Britton, Bruce K.},
    title={Lexical ambiguity of words used in English text.},
    journal={Behavior Research Methods {\&} Instrumentation},
    year={1978},
    publisher={Psychonomic Society},
    address={US},
    volume={10},
    number={1},
    pages={1-7},
    keywords={*Prose; *Vocabulary; Word Meaning},
    issn={0005-7878(Print)},
    doi={10.3758/BF03205079},
    url={https://doi.org/10.3758/BF03205079}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
}

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@article{chiu2020survey,
author = {Chiu, Billy and Baker, Simon},
year = {2020},
month = {12},
pages = {},
title = {Word embeddings for biomedical natural language processing: A survey},
volume = {14},
journal = {Language and Linguistics Compass},
doi = {10.1111/lnc3.12402}
}

@article{DBLP:journals/corr/abs-1211-5063,
  author       = {Razvan Pascanu and
                  Tom{\'{a}}s Mikolov and
                  Yoshua Bengio},
  title        = {Understanding the exploding gradient problem},
  journal      = {CoRR},
  volume       = {abs/1211.5063},
  year         = {2012},
  url          = {http://arxiv.org/abs/1211.5063},
  eprinttype    = {arXiv},
  eprint       = {1211.5063},
  timestamp    = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1211-5063.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2010-11929,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  journal      = {CoRR},
  volume       = {abs/2010.11929},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11929},
  eprinttype    = {arXiv},
  eprint       = {2010.11929},
  timestamp    = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{radford2018improving,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

@Article{Caucheteux2022,
    author={Caucheteux, Charlotte
    and King, Jean-R{\'e}mi},
    title={Brains and algorithms partially converge in natural language processing},
    journal={Communications Biology},
    year={2022},
    month={Feb},
    day={16},
    volume={5},
    number={1},
    pages={134},
    abstract={Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.},
    issn={2399-3642},
    doi={10.1038/s42003-022-03036-1},
    url={https://doi.org/10.1038/s42003-022-03036-1}
}

@inproceedings{hollenstein-etal-2019-cognival,
    title = "{C}ogni{V}al: A Framework for Cognitive Word Embedding Evaluation",
    author = "Hollenstein, Nora  and
      de la Torre, Antonio  and
      Langer, Nicolas  and
      Zhang, Ce",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1050",
    doi = "10.18653/v1/K19-1050",
    pages = "538--549",
    abstract = "An interesting method of evaluating word representations is by how much they reflect the semantic representations in the human brain. However, most, if not all, previous works only focus on small datasets and a single modality. In this paper, we present the first multi-modal framework for evaluating English word representations based on cognitive lexical semantics. Six types of word embeddings are evaluated by fitting them to 15 datasets of eye-tracking, EEG and fMRI signals recorded during language processing. To achieve a global score over all evaluation hypotheses, we apply statistical significance testing accounting for the multiple comparisons problem. This framework is easily extensible and available to include other intrinsic and extrinsic evaluation methods. We find strong correlations in the results between cognitive datasets, across recording modalities and to their performance on extrinsic NLP tasks.",
}

@misc{ wiki:zipflaw,
  author = "Wikipedia",
  title = "Zipf's law",
  year = "2023",
  url = "https://en.wikipedia.org/wiki/Zipf%27s_law",
}

@article{pietrini2001fmri,
author = {Haxby, James and Gobbini, Maria and Furey, Maura and Ishai, Alumit and Schouten, Jennifer and Pietrini, Pietro},
year = {2001},
month = {10},
pages = {2425-30},
title = {Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex},
volume = {293},
journal = {Science (New York, N.Y.)},
doi = {10.1126/science.1063736}
}

@article{
    doi:10.1073/pnas.96.16.9379,
    author = {Alumit Ishai  and Leslie G. Ungerleider  and Alex Martin  and Jennifer L. Schouten  and James V. Haxby },
    title = {Distributed representation of objects in the human ventral visual pathway},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {96},
    number = {16},
    pages = {9379-9384},
    year = {1999},
    doi = {10.1073/pnas.96.16.9379},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.96.16.9379},
    eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.96.16.9379},
    abstract = {Brain imaging and electrophysiological recording studies in humans have reported discrete cortical regions in posterior ventral temporal cortex that respond preferentially to faces, buildings, and letters. These findings suggest a category-specific anatomically segregated modular organization of the object vision pathway. Here we present data from a functional MRI study in which we found three distinct regions of ventral temporal cortex that responded preferentially to faces and two categories of other objects, namely houses and chairs, and had a highly consistent topological arrangement. Although the data could be interpreted as evidence for separate modules, we found that each category also evoked significant responses in the regions that responded maximally to other stimuli. Moreover, each category was associated with its own differential pattern of response across ventral temporal cortex. These results indicate that the representation of an object is not restricted to a region that responds maximally to that object, but rather is distributed across a broader expanse of cortex. We propose that the functional architecture of the ventral visual pathway is not a mosaic of category-specific modules but instead is a continuous representation of information about object form that has a highly consistent and orderly topological arrangement.}
}

@article{doi:10.1126/science.1152876,
    author = {Tom M. Mitchell  and Svetlana V. Shinkareva  and Andrew Carlson  and Kai-Min Chang  and Vicente L. Malave  and Robert A. Mason  and Marcel Adam Just },
    title = {Predicting Human Brain Activity Associated with the Meanings of Nouns},
    journal = {Science},
    volume = {320},
    number = {5880},
    pages = {1191-1195},
    year = {2008},
    doi = {10.1126/science.1152876},
    URL = {https://www.science.org/doi/abs/10.1126/science.1152876},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.1152876},
    abstract = {The question of how the human brain represents conceptual knowledge has been debated in many scientific fields. Brain imaging studies have shown that different spatial patterns of neural activation are associated with thinking about different semantic categories of pictures and words (for example, tools, buildings, and animals). We present a computational model that predicts the functional magnetic resonance imaging (fMRI) neural activation associated with words for which fMRI data are not yet available. This model is trained with a combination of data from a trillion-word text corpus and observed fMRI data associated with viewing several dozen concrete nouns. Once trained, the model predicts fMRI activation for thousands of other concrete nouns in the text corpus, with highly significant accuracies over the 60 nouns for which we currently have fMRI data.}
}

@Article{ friston1995fmri,
author = {K.J. Friston},
title = {Functional and effective connectivity in neuroimaging: A synthesis},
journal = {{H}uman {B}rain {M}apping},
year = {1995},
volume = {2},
pages = {56--78},
keyword = {connectivity} 
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}


@article{chersoni-etal-2021-decoding,
    title = "Decoding Word Embeddings with Brain-Based Semantic Features",
    author = "Chersoni, Emmanuele  and
      Santus, Enrico  and
      Huang, Chu-Ren  and
      Lenci, Alessandro",
    journal = "Computational Linguistics",
    volume = "47",
    number = "3",
    month = nov,
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.cl-3.20",
    doi = "10.1162/coli_a_00412",
    pages = "663--698",
    abstract = "Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these representations have been criticized for lacking interpretable dimensions. This property of word embeddings limits our understanding of the semantic features they actually encode. Moreover, it contributes to the {``}black box{''} nature of the tasks in which they are used, since the reasons for word embedding performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. 2016). Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the mapping in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the embedding performance and how they encode those features. This study sets itself as a step forward in understanding which aspects of meaning are captured by vector spaces, by proposing a new and simple method to carve human-interpretable semantic representations from distributional vectors.",
}

@article{wu2019gpt2,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@inbook{lecun95convolutional,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Lecun, Yann and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/274f964fb3cb53e8b758ada07b232cf48/schaul},
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  citeulike-article-id = {2380420},
  description = {idsia},
  editor = {Arbib, Michael A.},
  interhash = {445ed4af1eb7cca403276c34957f71f9},
  intrahash = {74f964fb3cb53e8b758ada07b232cf48},
  keywords = {evolutionary},
  pages = {255--258},
  priority = {2},
  publisher = {The MIT Press},
  timestamp = {2008-02-26T12:01:02.000+0100},
  title = {Convolutional Networks for Images, Speech and Time Series},
  year = 1995
}

@incollection{faasseckart2012,
  author = {Faass, Gertrud and Eckart, Kerstin},
  title = {SdeWaC -- A Corpus of Parsable Sentences from the Web},
  booktitle = {Language Processing and Knowledge in the Web},
  publisher = {Springer Berlin Heidelberg},
  year = {2013},
  editor = {Gurevych, Iryna and Biemann, Chris and Zesch, Torsten},
  volume = {8105},
  series = {Lecture Notes in Computer Science},
  pages = {61-68},
  doi = {10.1007/978-3-642-40722-2_6},
  isbn = {978-3-642-40721-5},
  keywords = {gcl,sfb732-b3}
}

@article{kaiser2022abstract,
    doi = {10.1371/journal.pcbi.1009837},
    author = {Kaiser, Daniel AND Jacobs, Arthur M. AND Cichy, Radoslaw M.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Modelling brain representations of abstract concepts},
    year = {2022},
    month = {02},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pcbi.1009837},
    pages = {1-15},
    abstract = {Abstract conceptual representations are critical for human cognition. Despite their importance, key properties of these representations remain poorly understood. Here, we used computational models of distributional semantics to predict multivariate fMRI activity patterns during the activation and contextualization of abstract concepts. We devised a task in which participants had to embed abstract nouns into a story that they developed around a given background context. We found that representations in inferior parietal cortex were predicted by concept similarities emerging in models of distributional semantics. By constructing different model families, we reveal the models’ learning trajectories and delineate how abstract and concrete training materials contribute to the formation of brain-like representations. These results inform theories about the format and emergence of abstract conceptual representations in the human brain.},
    number = {2},
}

@article{russell1989affect,
author = {Russell, James and Weiss, Anna and Mendelsohn, G.},
year = {1989},
month = {09},
pages = {493-502},
title = {Affect Grid: A Single-Item Scale of Pleasure and Arousal},
volume = {57},
journal = {Journal of Personality and Social Psychology},
doi = {10.1037/0022-3514.57.3.493}
}

@article{charles2018gyrus,
author = {Davis, Charles and Yee, Eiling},
year = {2018},
month = {05},
pages = {1-11},
title = {Features, labels, space, and time: factors supporting taxonomic relationships in the anterior temporal lobe and thematic relationships in the angular gyrus},
volume = {34},
journal = {Language, Cognition and Neuroscience},
doi = {10.1080/23273798.2018.1479530}
}

@article{price2015gyrus,
author = {Price, Amy and Bonner, Michael and Peelle, Jonathan and Grossman, Murray},
year = {2015},
month = {02},
pages = {3276-84},
title = {Converging Evidence for the Neuroanatomic Basis of Combinatorial Semantics in the Angular Gyrus},
volume = {35},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
doi = {10.1523/JNEUROSCI.3446-14.2015}
}

@misc{
    liao2023concept,
    title={Concept Understanding in Large Language Models: An Empirical Study},
    author={Jiayi Liao and Xu Chen and Lun Du},
    year={2023},
    url={https://openreview.net/forum?id=losgEaOWIL7}
}

@misc{santoro2022symbolic,
      title={Symbolic Behaviour in Artificial Intelligence}, 
      author={Adam Santoro and Andrew Lampinen and Kory Mathewson and Timothy Lillicrap and David Raposo},
      year={2022},
      eprint={2102.03406},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{remiking2022semantic,
author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Rémi},
year = {2022},
month = {09},
pages = {},
title = {Deep language algorithms predict semantic comprehension from brain activity},
volume = {12},
journal = {Scientific Reports},
doi = {10.1038/s41598-022-20460-9}
}

@inproceedings{ettinger2016probing,
    title = "Probing for semantic evidence of composition by means of simple classification tasks",
    author = "Ettinger, Allyson  and
      Elgohary, Ahmed  and
      Resnik, Philip",
    booktitle = "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP}",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2524",
    doi = "10.18653/v1/W16-2524",
    pages = "134--139",
}

@article{lenci2018distributional,
author = {Lenci, Alessandro},
year = {2018},
month = {02},
pages = {},
title = {Distributional Models of Word Meaning},
volume = {4},
journal = {Annual Review of Linguistics},
doi = {10.1146/annurev-linguistics-030514-125254}
}

@article{yee2016context,
author = {Yee, Eiling and Thompson-Schill, Sharon},
year = {2016},
month = {06},
pages = {1015-1027},
title = {Putting concepts into context},
volume = {23},
journal = {Psychonomic Bulletin & Review},
doi = {10.3758/s13423-015-0948-7}
}

@article{binder2016features,
author = {Binder, Jeffrey and Conant, Lisa and Humphries, Colin and Fernandino, Leonardo and Simons, Stephen and Aguilar, Mario and Desai, Rutvik},
year = {2016},
month = {06},
pages = {},
title = {Toward a brain-based componential semantic representation},
volume = {33},
journal = {Cognitive Neuropsychology},
doi = {10.1080/02643294.2016.1147426}
}

@inproceedings{10.5555/2969033.2969070,
    author = {Levy, Omer and Goldberg, Yoav},
    title = {Neural Word Embedding as Implicit Matrix Factorization},
    year = {2014},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
    booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
    pages = {2177–2185},
    numpages = {9},
    location = {Montreal, Canada},
    series = {NIPS'14}
}

@article{bojanowski-etal-2017-enriching,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1010",
    doi = "10.1162/tacl_a_00051",
    pages = "135--146",
    abstract = "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
}

@inproceedings{vulic-etal-2020-probing,
    title = "Probing Pretrained Language Models for Lexical Semantics",
    author = "Vuli{\'c}, Ivan  and
      Ponti, Edoardo Maria  and
      Litschko, Robert  and
      Glava{\v{s}}, Goran  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.586",
    doi = "10.18653/v1/2020.emnlp-main.586",
    pages = "7222--7240",
    abstract = "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
}

@book{vigliocco2007psycholinguistics,
    author = {Vigliocco, Gabriella, David P. Vinson, Gaskell and M. Gareth},
    title = "{The Oxford Handbook of Psycholinguistics}",
    publisher = {Oxford University Press},
    year = {2007},
    month = {08},
    abstract = "{This book examines the young science of psycholinguistics, which attempts to uncover the mechanisms and representations underlying human language. This interdisciplinary field has seen massive developments over the past decade, with a broad expansion of the research base, and the incorporation of new experimental techniques such as brain imaging and computational modelling. The result is that real progress is being made in the understanding of the key components of language in the mind. This book brings together the views of seventy-five leading researchers to provide a review of the current state of the art in psycholinguistics. The contributors are eminent in a wide range of fields, including psychology, linguistics, human memory, cognitive neuroscience, bilingualism, genetics, development, and neuropsychology. Their contributions are organised into six themed sections, covering word recognition, the mental lexicon, comprehension and discourse, language production, language development, and perspectives on psycholinguistics.}",
    isbn = {9780198568971},
    doi = {10.1093/oxfordhb/9780198568971.001.0001},
    url = {https://doi.org/10.1093/oxfordhb/9780198568971.001.0001},
    pages = {192-215},
}

@article{antonello2023discovery,
    author = {Antonello, Richard and Huth, Alexander},
    title = "{Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data}",
    journal = {Neurobiology of Language},
    pages = {1-16},
    year = {2023},
    month = {02},
    abstract = "{Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.}",
    issn = {2641-4368},
    doi = {10.1162/nol_a_00087},
    url = {https://doi.org/10.1162/nol\_a\_00087},
    eprint = {https://direct.mit.edu/nol/article-pdf/doi/10.1162/nol\_a\_00087/2072549/nol\_a\_00087.pdf},
}


@inproceedings{
    patel2022mapping,
    title={Mapping Language Models to Grounded Conceptual Spaces},
    author={Roma Patel and Ellie Pavlick},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=gJcEM8sxHK}
}

@misc{dziri2023faith,
      title={Faith and Fate: Limits of Transformers on Compositionality}, 
      author={Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jiang and Bill Yuchen Lin and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Sean Welleck and Xiang Ren and Allyson Ettinger and Zaid Harchaoui and Yejin Choi},
      year={2023},
      eprint={2305.18654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{
    wacongne2011predictive,
    author = {Catherine Wacongne  and Etienne Labyt  and Virginie van Wassenhove  and Tristan Bekinschtein  and Lionel Naccache  and Stanislas Dehaene },
    title = {Evidence for a hierarchy of predictions and prediction errors in human cortex},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {108},
    number = {51},
    pages = {20754-20759},
    year = {2011},
    doi = {10.1073/pnas.1117807108},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1117807108},
    eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1117807108},
    abstract = {According to hierarchical predictive coding models, the cortex constantly generates predictions of incoming stimuli at multiple levels of processing. Responses to auditory mismatches and omissions are interpreted as reflecting the prediction error when these predictions are violated. An alternative interpretation, however, is that neurons passively adapt to repeated stimuli. We separated these alternative interpretations by designing a hierarchical auditory novelty paradigm and recording human EEG and magnetoencephalographic (MEG) responses to mismatching or omitted stimuli. In the crucial condition, participants listened to frequent series of four identical tones followed by a fifth different tone, which generates a mismatch response. Because this response itself is frequent and expected, the hierarchical predictive coding hypothesis suggests that it should be cancelled out by a higher-order prediction. Three consequences ensue. First, the mismatch response should be larger when it is unexpected than when it is expected. Second, a perfectly monotonic sequence of five identical tones should now elicit a higher-order novelty response. Third, omitting the fifth tone should reveal the brain's hierarchical predictions. The rationale here is that, when a deviant tone is expected, its omission represents a violation of two expectations: a local prediction of a tone plus a hierarchically higher expectation of its deviancy. Thus, such an omission should induce a greater prediction error than when a standard tone is expected. Simultaneous EEE- magnetoencephalographic recordings verify those predictions and thus strongly support the predictive coding hypothesis. Higher-order predictions appear to be generated in multiple areas of frontal and associative cortices.}
}


@article{
    heilbron2022hierarchical,
    author = {Micha Heilbron  and Kristijan Armeni  and Jan-Mathijs Schoffelen  and Peter Hagoort  and Floris P. de Lange },
    title = {A hierarchy of linguistic predictions during natural language comprehension},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {119},
    number = {32},
    pages = {e2201968119},
    year = {2022},
    doi = {10.1073/pnas.2201968119},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2201968119},
    eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2201968119},
    abstract = {Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analyzing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable neural signatures of predictions about syntactic category (parts of speech), phonemes, and semantics. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.}
}

@article{caucheteux2023hierarchical,
author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Rémi},
year = {2023},
month = {03},
pages = {1-12},
title = {Evidence of a predictive coding hierarchy in the human brain listening to speech},
volume = {7},
journal = {Nature Human Behaviour},
doi = {10.1038/s41562-022-01516-2}
}

@inproceedings{alexnet2012,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    year = {2012},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
    booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
    pages = {1097–1105},
    numpages = {9},
    location = {Lake Tahoe, Nevada},
    series = {NIPS'12}
}

@article{kaplan2020scalinglaws,
  author       = {Jared Kaplan and
                  Sam McCandlish and
                  Tom Henighan and
                  Tom B. Brown and
                  Benjamin Chess and
                  Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Jeffrey Wu and
                  Dario Amodei},
  title        = {Scaling Laws for Neural Language Models},
  journal      = {CoRR},
  volume       = {abs/2001.08361},
  year         = {2020},
  url          = {https://arxiv.org/abs/2001.08361},
  eprinttype    = {arXiv},
  eprint       = {2001.08361},
  timestamp    = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{caucheteux2021disentangling,
      title={Disentangling Syntax and Semantics in the Brain with Deep Networks}, 
      author={Charlotte Caucheteux and Alexandre Gramfort and Jean-Remi King},
      year={2021},
      eprint={2103.01620},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{baroni2019compositionality,
  author       = {Marco Baroni},
  title        = {Linguistic generalization and compositionality in modern artificial
                  neural networks},
  journal      = {CoRR},
  volume       = {abs/1904.00157},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.00157},
  eprinttype    = {arXiv},
  eprint       = {1904.00157},
  timestamp    = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-00157.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gulordava-etal-2018-colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1108",
    doi = "10.18653/v1/N18-1108",
    pages = "1195--1205",
    abstract = "Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ({``}The colorless green ideas I ate with the chair sleep furiously{''}), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",
}

@inproceedings{lakretz-etal-2019-emergence,
    title = "The emergence of number and syntax units in {LSTM} language models",
    author = "Lakretz, Yair  and
      Kruszewski, German  and
      Desbordes, Theo  and
      Hupkes, Dieuwke  and
      Dehaene, Stanislas  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1002",
    doi = "10.18653/v1/N19-1002",
    pages = "11--20",
    abstract = "Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two {``}number units{''}. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.",
}

@misc{lake2018generalization,
      title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks}, 
      author={Brenden M. Lake and Marco Baroni},
      year={2018},
      eprint={1711.00350},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dessi-baroni-2019-cnns,
    title = "{CNN}s found to jump around more skillfully than {RNN}s: Compositional Generalization in Seq2seq Convolutional Networks",
    author = "Dess{\`\i}, Roberto  and
      Baroni, Marco",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1381",
    doi = "10.18653/v1/P19-1381",
    pages = "3919--3923",
    abstract = "Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of {``}jump around{''} 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.",
}

@misc{andreas2019measuring,
      title={Measuring Compositionality in Representation Learning}, 
      author={Jacob Andreas},
      year={2019},
      eprint={1902.07181},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ontanon-etal-2022-making,
    title = "Making Transformers Solve Compositional Tasks",
    author = "Ontanon, Santiago  and
      Ainslie, Joshua  and
      Fisher, Zachary  and
      Cvicek, Vaclav",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.251",
    doi = "10.18653/v1/2022.acl-long.251",
    pages = "3591--3607",
    abstract = "Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks. We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).",
}

@misc{turner2023activation,
      title={Activation Addition: Steering Language Models Without Optimization}, 
      author={Alexander Matt Turner and Lisa Thiergart and David Udell and Gavin Leech and Ulisse Mini and Monte MacDiarmid},
      year={2023},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wehbe-etal-2014-aligning,
    title = "Aligning context-based statistical models of language with brain activity during reading",
    author = "Wehbe, Leila  and
      Vaswani, Ashish  and
      Knight, Kevin  and
      Mitchell, Tom",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1030",
    doi = "10.3115/v1/D14-1030",
    pages = "233--243",
}

@article{utsumi2020,
author = {Utsumi, Akira},
year = {2020},
month = {06},
pages = {},
title = {Exploring What Is Encoded in Distributional Word Vectors: A Neurobiologically Motivated Analysis},
volume = {44},
journal = {Cognitive Science},
doi = {10.1111/cogs.12844}
}